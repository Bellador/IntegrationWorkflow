# -*- coding: utf-8 -*-
"""Transfer_Learning_Kreas_Blueprint.ipynb

Automatically generated by Colaboratory.

## Transfer Learning using Keras
"""

# import entire tensorflow
import tensorflow as tf
print(f'TensorFlow Version: {tf.__version__}')

# check GPU status
device_name = tf.test.gpu_device_name()
if device_name != '/device:GPU:0':
   raise SystemError('GPU device not found')
print('Found GPU at: {}'.format(device_name))
import os
os.system("nvidia-smi")

from tensorflow.compat.v1 import ConfigProto
from tensorflow.compat.v1 import InteractiveSession

config = ConfigProto()
config.gpu_options.per_process_gpu_memory_fraction = 0.5
config.gpu_options.allow_growth = True
session = InteractiveSession(config=config)

# import the libraries as shown below
from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, GlobalAveragePooling2D
from keras.utils.layer_utils import count_params
from tensorflow.keras.models import Model
from tensorflow.keras.applications.resnet50 import ResNet50
# from tensorflow.keras.applications.resnet101v2 import ResNet101V2
# from keras.applications.vgg16 import VGG16
# import tensorflow.keras.applications.ResNet101V2
from tensorflow.keras.applications.resnet50 import preprocess_input
from tensorflow.keras.preprocessing import image
from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing import image
import numpy as np
import math
from glob import glob
import matplotlib.pyplot as plt

# helper function to plot images for reference and verification
def plotImages(images_arr):
  fig, axes = plt.subplots(1, 8, figsize=(20, 20))
  axes = axes.flatten()
  for img, ax in zip(images_arr, axes):
    ax.imshow(img)
    ax.axis('off')
  plt.tight_layout()
  plt.show()

# helper function to calculate appropriate class weights on inbalanced sets
def create_class_weight(labels_dict,mu=0.15):
    total = np.sum(list(labels_dict.values()))
    keys = labels_dict.keys()
    class_weight = dict()
    
    for key in keys:
        score = math.log(mu*total/float(labels_dict[key]))
        class_weight[key] = score if score > 1.0 else 1.0
    
    return class_weight

# # check remaining time of this colab session
import time, psutil
Start = time.time() - psutil.boot_time()
Left = 12*3600 - Start
print('[*] time remaining for this session is: ', Left/3600)

# from google.colab import drive
# drive._mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# change to working tensorflow directory on the drive
# %cd /content/drive/MyDrive
# %ls -l

os.system("ls -l ./Datasets/train/not_red_kite/ | wc -l")
os.system("ls -l ./Datasets/test/not_red_kite/ | wc -l")
os.system("ls -l ./Datasets/train/red_kite/ | wc -l")
os.system("ls -l ./Datasets/test/red_kite/ | wc -l")

# dependent on the amount of images that need to be loaded from Google Drive the script executes to fast for the loading process to finish completely
# therefor inserting a stop assures this. Also check the loaded amount of images as backup check!
import time
# time.sleep(60)

# this function checks and deletes corrupt images on the Google drive that would throw errors during training
def check_corrupted_images():
  import io
  import os
  import cv2
  from PIL import Image

  bad_list = []
  dir_ = r"./Datasets"
  subdir_list = os.listdir(dir_)  # create a list of the sub directories in the directory ie train or test
  for d in subdir_list:  # iterate through the sub directories train and test
      dpath = os.path.join(dir_, d)  # create path to sub directory
      if d in ['test', 'train']:
          class_list = os.listdir(dpath)  # list of classes ie dog or cat
          # print (class_list)
          for class_ in class_list:  # iterate through the two classes
              if class_ == 'not_red_kite':
                class_path = os.path.join(dpath, class_)  # path to class directory
                # print(class_path)
                file_list = os.listdir(class_path)  # create list of files in class directory
                for f_index, f in enumerate(file_list, 1):  # iterate through the files
                    fpath = os.path.join(class_path, f)
                    index = f.rfind('.')  # find index of period infilename
                    ext = f[index + 1:]  # get the files extension
                    if ext not in ['jpg', 'png', 'bmp', 'gif']:
                        print(f'file {fpath}  has an invalid extension {ext}')
                        bad_list.append(fpath)
                    else:
                        try:
                            # test: UnidentifiedImageError: cannot identify image file <_io.BytesIO object at 0x7fde85991f50>
                            # fix: https://stackoverflow.com/questions/31077366/pil-cannot-identify-image-file-for-io-bytesio-object
                            byteImgIO = io.BytesIO()
                            byteImg = Image.open(fpath)
                            byteImg.save(byteImgIO, "PNG")
                            byteImgIO.seek(0)
                            byteImg = byteImgIO.read()
                        except Exception as e:
                            print(f'BytesIO: {e}')
                            bad_list.append(fpath)
                        try:
                            img = cv2.imread(fpath) # test to open with cv2
                            size = img.shape
                            try:
                                with Image.open(fpath) as im:
                                    width, height = im.size
                            except:
                                print(f'PIL: file {fpath} is not a valid image file ')
                                bad_list.append(fpath)
                        except:
                            print(f'CV2: file {fpath} is not a valid image file ')
                            bad_list.append(fpath)
                    print(f'\rdir: {d}, class: {class_}, file {f_index}', end='')
                      
  print(f'Bad list: f{bad_list}')
  return bad_list

#bad_list = check_corrupted_images()
#len(bad_list)
#for img in bad_list:
#    print(f'[!] bad img: {img}')

#time.sleep(60)
import os
# convert to set
#bad_list = set(bad_list)
# remove corrupt images
#for img in bad_list:
#  print(f'[!] remove img {img}')
#  try:
#    os.remove(img)
#  except Exception as e:
#    print(e)

# Commented out IPython magic to ensure Python compatibility.
# initialise tensorboard to monitor training
import datetime
# %load_ext tensorboard

# re-size all the images to this
IMAGE_SIZE = [224, 224] # should be based on input layer of chosen model
BATCH_SIZE = 64
EPOCHES = 200
LEARNING_RATE = 0.000001

# for class weight calculation; dictionary that contains training data per class 
# {'not_red_kite': 0, 'red_kite': 1}
labels_dict = {0: 27500, 1: 22749}
CHECKPOINT_SAVE_PATH = './Checkpoints/model.{epoch:02d}-{val_loss:.2f}.h5' # entire model is saved here, not only weights
MODEL_SAVE_PATH = './'

LOAD_MODEL_FROM_CHECKPOINT = True # if True, load a model already fitted by us previously
MODIFY_CHECKPOINT_MODEL = False # potentially alter the architecture of already trained, checkpoint model by adding e.g. another layer to train with different training data
CHECKPOINT_TO_LOAD = 'model.373-0.30.h5' # if LOAD_MODEL_FROM_CHECKPOINT = True this model will be loaded instead of the baseline model

train_path = './Datasets/train'
valid_path = './Datasets/test'

# useful for getting number of output classes
folders = glob(f'{train_path}/*')
print(f'[*] Nr. of classes: {len(folders)}\n[*] folders: {folders}')

# if the model was already trained previously than an existing ModelCheckpoint can be loaded here that will replace the above compiled model
if LOAD_MODEL_FROM_CHECKPOINT:
  print(f'[*] loading model from checkpoint...')
  model = load_model(f'./model_storage/{CHECKPOINT_TO_LOAD}')
else:
  print(f'[*] building model from scratch...')
  # Import the model library as shown below and add preprocessing layer to the front
  # Here we will be using imagenet weights
  MODEL_NAME = 'ResNet50'
  # Include_top lets you select if you want the final dense layers or not
  resnet = ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
  # resnet = tf.keras.applications.ResNet101V2(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)
  ## don't train existing weights
  for layer in resnet.layers:
      layer.trainable = False
  ## our layers - you can add more if you want
  x = Flatten()(resnet.output)
  #x = GlobalAveragePooling2D()(resnet.output)
  x = Dense(1024, activation='relu')(x) #1024, maybe 32? generally less dimensions?
  prediction = Dense(len(folders), activation='softmax')(x) # sigmoid; softmax - for multicathegorical classification (more than 2)
  # create a model object
  model = Model(inputs=resnet.input, outputs=prediction)
  # show trainable parameters with the new added layers and considering the frozen layers
  trainable_count = count_params(model.trainable_weights)
  print(f'trainable parameters: {trainable_count}')
  # define optimiser with custom learning rate
  opt = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)
  # tell the model what cost and optimization method to use
  model.compile(
    loss='categorical_crossentropy', # binary_crossentropy; categorical_crossentropy - for multicathegorical classification (more than 2) 
    optimizer=opt,
    metrics=['accuracy']
  )

# Use the Image Data Generator to import the images from the dataset
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255,
                                   shear_range=0.2,
                                   zoom_range=0.2,
                                   horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale=1./255)

# Make sure you provide the same target size as initialied for the image size
train_set = train_datagen.flow_from_directory(train_path,
                                                 target_size=IMAGE_SIZE,
                                                 batch_size=BATCH_SIZE,
                                                 class_mode='categorical', # 'categorical' 'binary'
                                                 shuffle=True,
                                                 seed=42)

len(train_set)
print(f'[*] len train_set: {len(train_set)}')
## plot first batch of augmented training data
#next_batch, next_labels = next(train_set)
#plotImages(next_batch)

test_set = test_datagen.flow_from_directory(valid_path,
                                            target_size=IMAGE_SIZE,
                                            batch_size=BATCH_SIZE,
                                            class_mode='categorical', # 'categorical' 'binary'
                                            )

label_map = (train_set.class_indices)
print(f'[*] label_map: {label_map}')
# set tensorboard callback
log_dir = "./logs/scalars/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")

tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir)
print(f'[*] log_dir: {log_dir}')

# define checkpoint
checkpoint = ModelCheckpoint(CHECKPOINT_SAVE_PATH, monitor='val_loss', verbose=1, save_best_only=True, mode='min')
# callbacks_list = [checkpoint] #  monitor='accuracy'

# calculate class weights based on class inbalance with sklearn
class_weigth_dict = create_class_weight(labels_dict)
class_weigth_dict

# Commented out IPython magic to ensure Python compatibility.
# %tensorboard --logdir ./logs/scalars

# fit the model
# Run the cell. It will take some time to execute
r = model.fit(
  train_set,
  epochs=EPOCHES,
  validation_data=test_set,
  steps_per_epoch=len(train_set),
  validation_steps=len(test_set),
  callbacks=[tensorboard_callback, checkpoint],
  class_weight=class_weigth_dict
)
print(f'[+] model training finished')

# plot the loss
plt.plot(r.history['loss'], label='train loss')
plt.plot(r.history['val_loss'], label='val loss')
plt.legend()
plt.show()
plt.savefig('LossVal_loss')

# plot the accuracy
plt.plot(r.history['accuracy'], label='train acc')
plt.plot(r.history['val_accuracy'], label='val acc')
plt.legend()
plt.show()
plt.savefig('AccVal_acc')

# save it as a h5 file
model.save(f'{MODEL_SAVE_PATH}/{MODEL_NAME}.h5')

# load model 
model = load_model(f'{MODEL_SAVE_PATH}/{MODEL_NAME}.h5')

test_loss, test_acc = model.evaluate(test_set)
print('Test accuracy:', test_acc)
print('Test loss:', test_loss)